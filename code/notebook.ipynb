{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated EEG Diagnosis Pipeline: Data Unification & Preprocessing\n",
    "\n",
    "This notebook consolidates the entire data processing workflow (Phases 0 and 1) for the Automated Multiclass Diagnosis of Neuropsychiatric Disorders project. It is designed to be run in a Kaggle environment to leverage more powerful CPU resources for the computationally intensive steps.\n",
    "\n",
    "Workflow:\n",
    "1.  Setup: Install libraries and define Kaggle-specific paths.\n",
    "2.  Phase 0.1: Metadata Unification: Scan raw dataset directories and create a single master_metadata.csv.\n",
    "3.  Phase 0.2: Data Validation: Perform a quality control check on all raw files.\n",
    "4.  Phase 1.1: Data Harmonization: Standardize all recordings to a common format (200Hz, 19 channels, Eyes-Closed).\n",
    "5.  Phase 1.2: Preprocessing: Apply filtering, automated ICA artifact removal, and epoching to create clean, analysis-ready data.\n",
    "6. Phase 2: Graph Construction: Transform preprocessed epochs into graph structures for the GNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install mne==1.6.1 mne-icalabel==0.6.0 ewtpy==1.0 mne-connectivity==0.7.0 torch-geometric==2.5.3\n",
    "\n",
    "# Import all necessary libraries for the entire notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # Use notebook-friendly tqdm\n",
    "import uuid\n",
    "import sys\n",
    "import mne\n",
    "from mne.io import Raw\n",
    "from mne_icalabel import label_components\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import ewtpy\n",
    "from scipy.stats import entropy\n",
    "import mne_connectivity\n",
    "import argparse\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Kaggle Path Configuration ---\n",
    "# IMPORTANT: Create a Kaggle Dataset and add it to this notebook.\n",
    "# The input path should point to the root of your dataset.\n",
    "KAGGLE_INPUT_DIR = Path('/kaggle/input/your-dataset-name')\n",
    "KAGGLE_WORKING_DIR = Path('/kaggle/working/')\n",
    "\n",
    "# Define all paths relative to the Kaggle environment\n",
    "BASE_DATA_PATH = KAGGLE_INPUT_DIR\n",
    "OUTPUT_DIR = KAGGLE_WORKING_DIR\n",
    "\n",
    "# Phase 0 Outputs\n",
    "MASTER_METADATA_PATH = OUTPUT_DIR / 'master_metadata.csv'\n",
    "VALIDATION_ERRORS_PATH = OUTPUT_DIR / 'validation_errors.csv'\n",
    "\n",
    "# Phase 1 Outputs\n",
    "HARMONIZED_DIR = OUTPUT_DIR / 'processed/harmonized'\n",
    "HARMONIZED_METADATA_PATH = OUTPUT_DIR / 'harmonized_metadata.csv'\n",
    "PREPROCESSED_DIR = OUTPUT_DIR / 'processed/preprocessed_epochs'\n",
    "PREPROCESSED_METADATA_PATH = OUTPUT_DIR / 'preprocessed_metadata.csv'\n",
    "SANITY_CHECK_DIR = OUTPUT_DIR / 'results/figures/ica_sanity_checks'\n",
    "\n",
    "# Phase 2 Outputs\n",
    "GRAPH_DIR = OUTPUT_DIR / 'processed/graphs'\n",
    "GRAPH_METADATA_PATH = OUTPUT_DIR / 'graph_metadata.csv'\n",
    "LABEL_MAP_PATH = OUTPUT_DIR / 'label_mapping.json'\n",
    "\n",
    "# Create output directories\n",
    "HARMONIZED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SANITY_CHECK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 0.1: Metadata Unification (create_metadata.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_main():\n",
    "    # --- Logic from create_metadata.py ---\n",
    "    def process_openneuro_ds004504():\n",
    "        dataset_path = BASE_DATA_PATH / 'ds004504'\n",
    "        participants_path = dataset_path / 'participants.tsv'\n",
    "        records = []\n",
    "        print('Processing OpenNeuro dataset...')\n",
    "        if not participants_path.exists():\n",
    "            print(f'  [ERROR] participants.tsv not found at {participants_path}')\n",
    "            return []\n",
    "        participants_df = pd.read_csv(participants_path, sep='\t')\n",
    "        for _, row in tqdm(participants_df.iterrows(), total=len(participants_df), desc='  -> OpenNeuro'):\n",
    "            subject_id = row['participant_id']\n",
    "            group_code = row['Group']\n",
    "            label_map = {'C': 'CN', 'A': 'AD', 'F': 'FTD'}\n",
    "            label = label_map.get(group_code)\n",
    "            if label:\n",
    "                eeg_file_path = dataset_path / 'raw_data-edf_converted' / subject_id / 'eeg' / f'{subject_id}task-eyesclosed_eeg.edf'\n",
    "                if eeg_file_path.exists():\n",
    "                    records.append({'original_subject_id': subject_id, 'file_path': str(eeg_file_path.resolve()), 'diagnosis': label, 'original_dataset_source': 'ds004504', 'age': row.get('Age'), 'sex': row.get('Gender'), 'sampling_rate': 500})\n",
    "        return records\n",
    "\n",
    "    def process_caueeg():\n",
    "        dataset_path = BASE_DATA_PATH / 'caueeg'\n",
    "        annotation_path = dataset_path / 'annotation.json'\n",
    "        signal_path = dataset_path / 'signal' / 'edf'\n",
    "        records = []\n",
    "        print('Processing CAUEEG dataset...')\n",
    "        if not annotation_path.exists() or not signal_path.exists(): return []\n",
    "        with open(annotation_path, 'r') as f: master_meta = json.load(f)\n",
    "        subject_lookup = {item['serial']: item for item in master_meta['data']}\n",
    "        for edf_file in tqdm(list(signal_path.glob('*.edf')), desc='  -> CAUEEG'):\n",
    "            original_subject_id = edf_file.stem\n",
    "            subject_data = subject_lookup.get(original_subject_id)\n",
    "            if not subject_data: continue\n",
    "            symptoms = subject_data.get('symptom', [])\n",
    "            label = None\n",
    "            if any(tag in symptoms for tag in ['dementia', 'ad', 'load']): label = 'AD'\n",
    "            elif any(tag in symptoms for tag in ['mci', 'mci_amnestic', 'mci_amnestic_rf']): label = 'MCI'\n",
    "            elif any(tag in symptoms for tag in ['normal', 'cb_normal']): label = 'CN'\n",
    "            if label:\n",
    "                records.append({'original_subject_id': original_subject_id, 'file_path': str(edf_file.resolve()), 'diagnosis': label, 'original_dataset_source': 'caueeg', 'age': subject_data.get('age'), 'sex': subject_data.get('gender'), 'sampling_rate': 200})\n",
    "        return records\n",
    "\n",
    "    def process_figshare_mdd():\n",
    "        dataset_path = BASE_DATA_PATH / 'figshare_mdd'\n",
    "        records = []\n",
    "        print('Processing Figshare MDD dataset...')\n",
    "        if not dataset_path.exists(): return []\n",
    "        for edf_file in tqdm(list(dataset_path.glob('* Subjects/*EC.edf')), desc='  -> Figshare'):\n",
    "            filename = edf_file.stem\n",
    "            label = 'MDD' if 'MDD' in filename else 'CN' if 'H S' in filename else None\n",
    "            if label:\n",
    "                records.append({'original_subject_id': filename, 'file_path': str(edf_file.resolve()), 'diagnosis': label, 'original_dataset_source': 'figshare_mdd', 'age': None, 'sex': None, 'sampling_rate': 256})\n",
    "        return records\n",
    "\n",
    "    print('--- Phase 0.1: Master Metadata Generation ---')\n",
    "    all_records = []\n",
    "    all_records.extend(process_openneuro_ds004504())\n",
    "    all_records.extend(process_caueeg())\n",
    "    all_records.extend(process_figshare_mdd())\n",
    "    if not all_records: return\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df['subject_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    column_order = ['subject_id', 'original_subject_id', 'diagnosis', 'file_path', 'original_dataset_source', 'sampling_rate', 'age', 'sex']\n",
    "    df = df[column_order]\n",
    "    df.to_csv(MASTER_METADATA_PATH, index=False)\n",
    "    print(f'\n",
    "--- Metadata Generation Complete ---')\n",
    "    print(f'Successfully processed {len(df)} records.')\n",
    "    print('\n",
    "Final Class Distribution:\n",
    "', df['diagnosis'].value_counts())\n",
    "    print(f'\n",
    "✅ Master metadata file saved to: {MASTER_METADATA_PATH}')\n",
    "\n",
    "create_metadata_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 0.2: Data Validation (validate_data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_main():\n",
    "    # --- Logic from validate_data.py ---\n",
    "    REQUIRED_CHANNELS = {'Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz'}\n",
    "    print('\n",
    "--- Phase 0.2: Data Validation ---')\n",
    "    if not MASTER_METADATA_PATH.exists():\n",
    "        print('[FATAL] Metadata file not found.')\n",
    "        return\n",
    "    metadata_df = pd.read_csv(MASTER_METADATA_PATH)\n",
    "    success_count = 0\n",
    "    error_records = []\n",
    "    for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc='Validating files'):\n",
    "        file_path = Path(row['file_path'])\n",
    "        error_messages = []\n",
    "        if not file_path.exists():\n",
    "            error_messages.append('File not found.')\n",
    "        else:\n",
    "            try:\n",
    "                raw = mne.io.read_raw_edf(file_path, preload=False, verbose='CRITICAL')\n",
    "                if int(raw.info['sfreq']) != row['sampling_rate']:\n",
    "                    error_messages.append(f'SR mismatch: Expected {row['sampling_rate']}, found {int(raw.info['sfreq'])}')\n",
    "                available_channels_upper = {ch.upper() for ch in raw.ch_names}\n",
    "                missing = {ch for ch in REQUIRED_CHANNELS if not any(ch in actual for actual in available_channels_upper)}\n",
    "                if missing: error_messages.append(f'Missing channels: {', '.join(sorted(list(missing)))}')\n",
    "            except Exception as e: error_messages.append(f'MNE read error: {e}')\n",
    "        if not error_messages: success_count += 1\n",
    "        else: error_records.append({'subject_id': row['subject_id'], 'file_path': str(file_path), 'errors': '; '.join(error_messages)})\n",
    "    print('\n",
    "--- Validation Complete ---')\n",
    "    print(f'Successfully validated: {success_count} / {len(metadata_df)} files.')\n",
    "    if error_records:\n",
    "        pd.DataFrame(error_records).to_csv(VALIDATION_ERRORS_PATH, index=False)\n",
    "        print(f'Found {len(error_records)} issues. See: {VALIDATION_ERRORS_PATH}')\n",
    "    else: print('\n",
    "✅ All files passed validation!')\n",
    "\n",
    "validate_data_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1.1: Data Harmonization (harmonize_data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_data_main():\n",
    "    # --- Logic from harmonize_data.py ---\n",
    "    TARGET_SAMPLING_RATE = 200\n",
    "    STANDARD_19_CHANNELS = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "\n",
    "    def rename_channels(raw: Raw) -> Raw:\n",
    "        rename_mapping = {}\n",
    "        ch_names_upper = [ch.upper() for ch in raw.ch_names]\n",
    "        for standard_ch in STANDARD_19_CHANNELS:\n",
    "            for i, actual_ch_upper in enumerate(ch_names_upper):\n",
    "                if standard_ch.upper() in actual_ch_upper: rename_mapping[raw.ch_names[i]] = standard_ch; break\n",
    "        raw.rename_channels(rename_mapping)\n",
    "        return raw\n",
    "\n",
    "    def extract_eyes_closed_caueeg(raw: Raw, subject_id: str) -> Raw:\n",
    "        event_json_path = BASE_DATA_PATH / 'caueeg' / 'event' / f'{subject_id}.json'\n",
    "        if not event_json_path.exists(): return raw\n",
    "        with open(event_json_path, 'r') as f: events = json.load(f)\n",
    "        ec_segments = []\n",
    "        max_time = raw.times[-1]\n",
    "        for i, (timestamp, desc) in enumerate(events):\n",
    "            if 'eyes closed' in desc.lower():\n",
    "                start_time_sec = timestamp / raw.info['sfreq']\n",
    "                end_time_sec = max_time\n",
    "                if i + 1 < len(events): end_time_sec = events[i+1][0] / raw.info['sfreq']\n",
    "                end_time_sec = min(end_time_sec, max_time)\n",
    "                if end_time_sec - start_time_sec > 1e-5: ec_segments.append(raw.copy().crop(tmin=start_time_sec, tmax=end_time_sec))\n",
    "        if not ec_segments: return raw.crop(tmax=0)\n",
    "        return mne.concatenate_raws(ec_segments)\n",
    "\n",
    "    print('\n",
    "--- Phase 1.1: Data Harmonization ---')\n",
    "    if not MASTER_METADATA_PATH.exists():\n",
    "        print('[FATAL] Master metadata not found.')\n",
    "        return\n",
    "    metadata_df = pd.read_csv(MASTER_METADATA_PATH)\n",
    "    harmonized_records = []\n",
    "    for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc='Harmonizing files'):\n",
    "        try:\n",
    "            raw: Raw = mne.io.read_raw_edf(row['file_path'], preload=True, verbose='WARNING')\n",
    "            raw.set_meas_date(None)\n",
    "            raw = rename_channels(raw)\n",
    "            raw.pick(STANDARD_19_CHANNELS, ordered=True)\n",
    "            if row['original_dataset_source'] == 'caueeg':\n",
    "                raw = extract_eyes_closed_caueeg(raw, row['original_subject_id'])\n",
    "            if raw.times.size == 0: continue\n",
    "            if raw.info['sfreq'] != TARGET_SAMPLING_RATE:\n",
    "                raw.resample(TARGET_SAMPLING_RATE, verbose='WARNING')\n",
    "            sanitized_id = str(row['original_subject_id']).replace(' ', '').replace('-', '')\n",
    "            output_filename = f'{row['original_dataset_source']}harmonized{row['diagnosis']}{sanitized_id}eeg.fif'\n",
    "            output_filepath = HARMONIZED_DIR / output_filename\n",
    "            raw.save(output_filepath, overwrite=True, verbose='WARNING')\n",
    "            new_record = row.to_dict()\n",
    "            new_record['harmonized_file_path'] = str(output_filepath.resolve())\n",
    "            harmonized_records.append(new_record)\n",
    "        except Exception as e: print(f'\n",
    "[ERROR] Failed on {row['file_path']}. Reason: {e}')\n",
    "    if harmonized_records:\n",
    "        harmonized_df = pd.DataFrame(harmonized_records)\n",
    "        harmonized_df.to_csv(HARMONIZED_METADATA_PATH, index=False)\n",
    "        print('\n",
    "--- Harmonization Complete ---')\n",
    "        print(f'Successfully processed {len(harmonized_records)} files.')\n",
    "        print(f'✅ New metadata saved to: {HARMONIZED_METADATA_PATH}')\n",
    "\n",
    "harmonize_data_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1.2: Preprocessing (preprocess_data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_main():\n",
    "    # --- Logic from preprocess_data.py ---\n",
    "    BANDPASS_FREQ = (1.0, 45.0)\n",
    "    NOTCH_FREQ = 50.0\n",
    "    EPOCH_DURATION = 2.0\n",
    "    REJECT_CRITERIA = dict(eeg=100e-6)\n",
    "    ICA_N_COMPONENTS = 15\n",
    "    ICA_RANDOM_STATE = 42\n",
    "    ICA_METHOD = 'infomax'\n",
    "    ICA_FIT_PARAMS = dict(extended=True)\n",
    "    ICA_FILTER_FREQ = (1.0, 100.0)\n",
    "\n",
    "    def preprocess_subject(file_path: Path) -> Tuple[Optional[mne.Epochs], Optional[mne.preprocessing.ICA]]:\n",
    "        try:\n",
    "            raw: Raw = mne.io.read_raw_fif(file_path, preload=True, verbose='WARNING')\n",
    "            raw.set_montage('standard_1020', on_missing='raise', verbose='WARNING')\n",
    "            raw_for_ica = raw.copy().filter(l_freq=ICA_FILTER_FREQ[0], h_freq=ICA_FILTER_FREQ[1], verbose='WARNING')\n",
    "            raw.filter(l_freq=BANDPASS_FREQ[0], h_freq=BANDPASS_FREQ[1], verbose='WARNING')\n",
    "            raw.notch_filter(freqs=NOTCH_FREQ, verbose='WARNING')\n",
    "            raw.set_eeg_reference('average', projection=False, verbose='WARNING')\n",
    "            ica = mne.preprocessing.ICA(n_components=ICA_N_COMPONENTS, method=ICA_METHOD, fit_params=ICA_FIT_PARAMS, random_state=ICA_RANDOM_STATE)\n",
    "            ica.fit(raw_for_ica)\n",
    "            component_labels = label_components(raw, ica, method='iclabel')\n",
    "            ica.exclude = [idx for idx, label in enumerate(component_labels['labels']) if label not in ['brain', 'other']]\n",
    "            epochs = mne.make_fixed_length_epochs(raw, duration=EPOCH_DURATION, preload=True, verbose='WARNING')\n",
    "            ica.apply(epochs, verbose='WARNING')\n",
    "            epochs.drop_bad(reject=REJECT_CRITERIA, verbose='WARNING')\n",
    "            return epochs, ica\n",
    "        except Exception as e: print(f'\n",
    "[ERROR] Could not process {file_path}. Reason: {e}'); return None, None\n",
    "\n",
    "    print('\n",
    "--- Phase 1.2: Preprocessing ---')\n",
    "    if not HARMONIZED_METADATA_PATH.exists():\n",
    "        print('[FATAL] Harmonized metadata not found.')\n",
    "        return\n",
    "    metadata_df = pd.read_csv(HARMONIZED_METADATA_PATH)\n",
    "    preprocessed_records = []\n",
    "    for idx, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc='Preprocessing files'):\n",
    "        epochs, ica = preprocess_subject(Path(row['harmonized_file_path']))\n",
    "        if epochs is not None and len(epochs) > 0:\n",
    "            sanitized_id = str(row['original_subject_id']).replace(' ', '').replace('-', '')\n",
    "            output_filename = f'{row['original_dataset_source']}preprocessed{row['diagnosis']}_{sanitized_id}_epo.fif'\n",
    "            output_filepath = PREPROCESSED_DIR / output_filename\n",
    "            epochs.save(output_filepath, overwrite=True, verbose='WARNING')\n",
    "            new_record = row.to_dict()\n",
    "            new_record['preprocessed_epo_path'] = str(output_filepath.resolve())\n",
    "            new_record['n_clean_epochs'] = len(epochs)\n",
    "            preprocessed_records.append(new_record)\n",
    "    if preprocessed_records:\n",
    "        preprocessed_df = pd.DataFrame(preprocessed_records)\n",
    "        preprocessed_df.to_csv(PREPROCESSED_METADATA_PATH, index=False)\n",
    "        print('\n",
    "--- Preprocessing Complete ---')\n",
    "        print(f'Successfully processed {len(preprocessed_records)} files.')\n",
    "        print(f'✅ New metadata saved to: {PREPROCESSED_METADATA_PATH}')\n",
    "\n",
    "preprocess_data_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Graph Construction (create_graphs.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graphs_main():\n",
    "    # --- Logic from create_graphs.py ---\n",
    "    SAMPLING_RATE = 200\n",
    "    BANDS = {\"Delta\": (1, 4), \"Theta\": (4, 8), \"Alpha\": (8, 13), \"Beta\": (13, 30), \"Gamma\": (30, 45)}\n",
    "\n",
    "    def shannon_entropy(data: np.ndarray) -> float:\n",
    "        \"\"\"Calculates the Shannon Entropy of a signal.\"\"\"\n",
    "        counts, _ = np.histogram(data, bins='auto', density=True)\n",
    "        # Filter out zero probabilities to avoid log(0)\n",
    "        counts = counts[counts > 0]\n",
    "        return entropy(counts) # type: ignore\n",
    "\n",
    "    def calculate_node_features(epoch_data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates node features for a single epoch using EWT.\"\"\"\n",
    "        n_channels, _ = epoch_data.shape\n",
    "        node_features = np.zeros((n_channels, len(BANDS) * 2))\n",
    "        for i in range(n_channels):\n",
    "            channel_signal = epoch_data[i, :]\n",
    "            ewt, _, _ = ewtpy.EWT1D(channel_signal, N=len(BANDS))\n",
    "            for band_idx in range(ewt.shape[1]):\n",
    "                sub_band = ewt[:, band_idx]\n",
    "                node_features[i, band_idx * 2] = np.log1p(np.var(sub_band))\n",
    "                node_features[i, (band_idx * 2) + 1] = shannon_entropy(sub_band)\n",
    "        return node_features\n",
    "\n",
    "    def calculate_wpli_connectivity(epochs: mne.Epochs) -> np.ndarray:\n",
    "        \"\"\"Calculates the broadband wPLI connectivity matrix for all epochs.\"\"\"\n",
    "        conn = mne_connectivity.spectral_connectivity_epochs(\n",
    "            epochs, method='wpli', mode='multitaper',\n",
    "            fmin=BANDS[\"Delta\"][0], fmax=BANDS[\"Gamma\"][1],\n",
    "            faverage=True, verbose=False\n",
    "        )\n",
    "        return conn.get_data(output='dense').squeeze()\n",
    "\n",
    "    print('\\n--- Phase 2: Graph Construction ---')\n",
    "    if not PREPROCESSED_METADATA_PATH.exists():\n",
    "        print('[FATAL] Preprocessed metadata not found.')\n",
    "        return\n",
    "    metadata_df = pd.read_csv(PREPROCESSED_METADATA_PATH)\n",
    "    unique_labels = sorted(metadata_df['diagnosis'].unique())\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    with open(LABEL_MAP_PATH, 'w') as f: json.dump(label_map, f, indent=4)\n",
    "    print(f'✅ Saved label mapping to {LABEL_MAP_PATH}')\n",
    "\n",
    "    graph_records = []\n",
    "    for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc='Creating graphs'):\n",
    "        try:\n",
    "            epochs = mne.read_epochs(row['preprocessed_epo_path'], preload=True, verbose=False)\n",
    "            wpli_matrices = calculate_wpli_connectivity(epochs)\n",
    "            if len(epochs) == 1: wpli_matrices = np.expand_dims(wpli_matrices, axis=0)\n",
    "\n",
    "            for i in range(len(epochs)):\n",
    "                node_features = calculate_node_features(epochs[i].get_data(copy=False).squeeze())\n",
    "                adj_matrix = wpli_matrices[i, :, :]\n",
    "                edge_index, edge_attr = dense_to_sparse(torch.tensor(adj_matrix, dtype=torch.float))\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "                y = torch.tensor([label_map[row['diagnosis']]], dtype=torch.long)\n",
    "                graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "                \n",
    "                graph_filename = f\"{Path(row['preprocessed_epo_path']).stem}_graph_{i}.pt\"\n",
    "                graph_filepath = GRAPH_DIR / graph_filename\n",
    "                torch.save(graph_data, graph_filepath)\n",
    "                \n",
    "                graph_records.append({\n",
    "                    'subject_id': row['subject_id'], \n",
    "                    'diagnosis': row['diagnosis'], \n",
    "                    'graph_file_path': str(graph_filepath.resolve())\n",
    "                })\n",
    "        except Exception as e: print(f'\\n[ERROR] Failed on {row['subject_id']}. Reason: {e}')\n",
    "    \n",
    "    # --- Completion of the cell logic ---\n",
    "    if graph_records:\n",
    "        graph_df = pd.DataFrame(graph_records)\n",
    "        graph_df.to_csv(GRAPH_METADATA_PATH, index=False)\n",
    "        print(\"\\n--- Graph Construction Complete ---\")\n",
    "        print(f\"Successfully created {len(graph_df)} graph files.\")\n",
    "        print(f\"✅ New graph metadata saved to: {GRAPH_METADATA_PATH}\")\n",
    "    else:\n",
    "        print(\"\\n--- Graph Construction Failed ---\")\n",
    "        print(\"No graph files were generated.\")\n",
    "\n",
    "# Execute the main function for this phase\n",
    "create_graphs_main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
